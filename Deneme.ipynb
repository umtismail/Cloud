{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"jp-MarkdownHeadingCollapsed": true,
				"tags": [],
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.5 \n",
					"output_type": "stream"
				},
				{
					"output_type": "display_data",
					"data": {
						"text/markdown": "\n# Available Magic Commands\n\n## Sessions Magic\n\n----\n    %help                             Return a list of descriptions and input types for all magic commands. \n    %profile            String        Specify a profile in your aws configuration to use as the credentials provider.\n    %region             String        Specify the AWS region in which to initialize a session. \n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\ USERNAME \\.aws\\config\" on Windows.\n    %idle_timeout       Int           The number of minutes of inactivity after which a session will timeout. \n                                      Default: 2880 minutes (48 hours).\n    %timeout            Int           The number of minutes after which a session will timeout. \n                                      Default: 2880 minutes (48 hours).\n    %session_id_prefix  String        Define a String that will precede all session IDs in the format \n                                      [session_id_prefix]-[session_id]. If a session ID is not provided,\n                                      a random UUID will be generated.\n    %status                           Returns the status of the current Glue session including its duration, \n                                      configuration and executing user / role.\n    %session_id                       Returns the session ID for the running session.\n    %list_sessions                    Lists all currently running sessions by ID.\n    %stop_session                     Stops the current session.\n    %glue_version       String        The version of Glue to be used by this session. \n                                      Currently, the only valid options are 2.0, 3.0 and 4.0. \n                                      Default: 2.0.\n    %reconnect          String        Specify a live session ID to switch/reconnect to the sessions.\n----\n\n## Selecting Session Types\n\n----\n    %streaming          String        Sets the session type to Glue Streaming.\n    %etl                String        Sets the session type to Glue ETL.\n    %glue_ray           String        Sets the session type to Glue Ray.\n    %session_type       String        Specify a session_type to be used. Supported values: streaming, etl and glue_ray. \n----\n\n## Glue Config Magic \n*(common across all session types)*\n\n----\n\n    %%configure         Dictionary    A json-formatted dictionary consisting of all configuration parameters for \n                                      a session. Each parameter can be specified here or through individual magics.\n    %iam_role           String        Specify an IAM role ARN to execute your session with.\n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\%USERNAME%\\.aws\\config` on Windows.\n    %number_of_workers  int           The number of workers of a defined worker_type that are allocated \n                                      when a session runs.\n                                      Default: 5.\n    %additional_python_modules  List  Comma separated list of additional Python modules to include in your cluster \n                                      (can be from Pypi or S3).\n    %%tags        Dictionary          Specify a json-formatted dictionary consisting of tags to use in the session.\n    \n    %%assume_role Dictionary, String  Specify a json-formatted dictionary or an IAM role ARN string to create a session \n                                      for cross account access.\n                                      E.g. {valid arn}\n                                      %%assume_role \n                                      'arn:aws:iam::XXXXXXXXXXXX:role/AWSGlueServiceRole' \n                                      E.g. {credentials}\n                                      %%assume_role\n                                      {\n                                            \"aws_access_key_id\" : \"XXXXXXXXXXXX\",\n                                            \"aws_secret_access_key\" : \"XXXXXXXXXXXX\",\n                                            \"aws_session_token\" : \"XXXXXXXXXXXX\"\n                                       }\n----\n\n                                      \n## Magic for Spark Sessions (ETL & Streaming)\n\n----\n    %worker_type        String        Set the type of instances the session will use as workers. \n    %connections        List          Specify a comma separated list of connections to use in the session.\n    %extra_py_files     List          Comma separated list of additional Python files From S3.\n    %extra_jars         List          Comma separated list of additional Jars to include in the cluster.\n    %spark_conf         String        Specify custom spark configurations for your session. \n                                      E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n----\n                                      \n## Magic for Ray Session\n\n----\n    %min_workers        Int           The minimum number of workers that are allocated to a Ray session. \n                                      Default: 1.\n    %object_memory_head Int           The percentage of free memory on the instance head node after a warm start. \n                                      Minimum: 0. Maximum: 100.\n    %object_memory_worker Int         The percentage of free memory on the instance worker nodes after a warm start. \n                                      Minimum: 0. Maximum: 100.\n----\n\n## Action Magic\n\n----\n\n    %%sql               String        Run SQL code. All lines after the initial %%sql magic will be passed\n                                      as part of the SQL code.  \n    %matplot      Matplotlib figure   Visualize your data using the matplotlib library.\n                                      E.g. \n                                      import matplotlib.pyplot as plt\n                                      # Set X-axis and Y-axis values\n                                      x = [5, 2, 8, 4, 9]\n                                      y = [10, 4, 8, 5, 2]\n                                      # Create a bar chart \n                                      plt.bar(x, y) \n                                      # Show the plot\n                                      %matplot plt    \n    %plotly            Plotly figure  Visualize your data using the plotly library.\n                                      E.g.\n                                      import plotly.express as px\n                                      #Create a graphical figure\n                                      fig = px.line(x=[\"a\",\"b\",\"c\"], y=[1,3,2], title=\"sample figure\")\n                                      #Show the figure\n                                      %plotly fig\n\n  \n                \n----\n\n"
					},
					"metadata": {}
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.5 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 5\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nIdle Timeout: 2880\nSession ID: f26fbff3-3ff9-43d7-b5d8-c14a59510795\nApplying the following default arguments:\n--glue_kernel_version 1.0.5\n--enable-glue-datacatalog true\nWaiting for session f26fbff3-3ff9-43d7-b5d8-c14a59510795 to get into ready status...\nSession f26fbff3-3ff9-43d7-b5d8-c14a59510795 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
			"metadata": {
				"editable": true,
				"tags": [],
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "dyf = glueContext.create_dynamic_frame.from_catalog(database='database_name', table_name='table_name')\ndyf.printSchema()",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nIdle Timeout: 2880\nSession ID: 9f9b6664-ee98-44c2-8394-ad27610164c2\nApplying the following default arguments:\n--glue_kernel_version 1.0.5\n--enable-glue-datacatalog true\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "Following exception encountered while creating session: An error occurred (AlreadyExistsException) when calling the CreateSession operation: Session already created, sessionId=9f9b6664-ee98-44c2-8394-ad27610164c2 \n\nError message: Session already created, sessionId=9f9b6664-ee98-44c2-8394-ad27610164c2 \n\nTraceback (most recent call last):\n  File \"/home/jupyter-user/.local/lib/python3.9/site-packages/aws_glue_interactive_sessions_kernel/glue_kernel_utils/KernelGateway.py\", line 100, in create_session\n    response = self.glue_client.create_session(\n  File \"/home/jupyter-user/.local/lib/python3.9/site-packages/botocore/client.py\", line 553, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File \"/home/jupyter-user/.local/lib/python3.9/site-packages/botocore/client.py\", line 1009, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.errorfactory.AlreadyExistsException: An error occurred (AlreadyExistsException) when calling the CreateSession operation: Session already created, sessionId=9f9b6664-ee98-44c2-8394-ad27610164c2\nException encountered while creating session: An error occurred (AlreadyExistsException) when calling the CreateSession operation: Session already created, sessionId=9f9b6664-ee98-44c2-8394-ad27610164c2 \nTraceback (most recent call last):\n  File \"/home/jupyter-user/.local/lib/python3.9/site-packages/aws_glue_interactive_sessions_kernel/glue_kernel_base/BaseKernel.py\", line 170, in do_execute\n    self.create_session()\n  File \"/home/jupyter-user/.local/lib/python3.9/site-packages/aws_glue_interactive_sessions_kernel/glue_kernel_base/BaseKernel.py\", line 611, in create_session\n    response = self.kernel_gateway.create_session(\n  File \"/home/jupyter-user/.local/lib/python3.9/site-packages/aws_glue_interactive_sessions_kernel/glue_kernel_utils/KernelGateway.py\", line 100, in create_session\n    response = self.glue_client.create_session(\n  File \"/home/jupyter-user/.local/lib/python3.9/site-packages/botocore/client.py\", line 553, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File \"/home/jupyter-user/.local/lib/python3.9/site-packages/botocore/client.py\", line 1009, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.errorfactory.AlreadyExistsException: An error occurred (AlreadyExistsException) when calling the CreateSession operation: Session already created, sessionId=9f9b6664-ee98-44c2-8394-ad27610164c2\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, regexp_extract\n\n# Initialize a Spark Session\nspark = SparkSession.builder.appName('openaq_processing').getOrCreate()\n\n# Read from S3 bucket\ndata_location = \"s3://myawasbuckt-airquality/openaq/air_quality_data.json\"\ndf = spark.read.json(data_location)\n\n# Drop the original 'city' column and rename 'location' to 'city'\ndf = df.drop(\"city\")\ndf = df.withColumnRenamed(\"location\", \"city\")\n\n# Filter out cities with names that seem coded (like MK0036A)\n# This will ensure that if there's a sequence of a letter followed by a number anywhere in the city name, it's excluded.\ndf = df.filter(~col(\"city\").rlike(\"[A-Z]+[0-9]+\"))\n\n# Show the schema and sample data\ndf.printSchema()\ndf.show(5, truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- coordinates: struct (nullable = true)\n |    |-- latitude: double (nullable = true)\n |    |-- longitude: double (nullable = true)\n |-- country: string (nullable = true)\n |-- date: struct (nullable = true)\n |    |-- local: string (nullable = true)\n |    |-- utc: string (nullable = true)\n |-- entity: string (nullable = true)\n |-- isAnalysis: string (nullable = true)\n |-- isMobile: boolean (nullable = true)\n |-- city: string (nullable = true)\n |-- locationId: long (nullable = true)\n |-- parameter: string (nullable = true)\n |-- sensorType: string (nullable = true)\n |-- unit: string (nullable = true)\n |-- value: double (nullable = true)\n\n+--------------------------------------+-------+------------------------------------------------------+-------------------------+----------+--------+-------------------+----------+---------+---------------+-----+-----+\n|coordinates                           |country|date                                                  |entity                   |isAnalysis|isMobile|city               |locationId|parameter|sensorType     |unit |value|\n+--------------------------------------+-------+------------------------------------------------------+-------------------------+----------+--------+-------------------+----------+---------+---------------+-----+-----+\n|{52.11162099964508, 5.708418999868416}|NL     |{2024-07-08T16:00:00+02:00, 2024-07-08T14:00:00+00:00}|Governmental Organization|null      |false   |Wekerom-Riemterdijk|31        |pm25     |reference grade|µg/m³|5.436|\n|{52.11162099964508, 5.708418999868416}|NL     |{2024-07-08T15:00:00+02:00, 2024-07-08T13:00:00+00:00}|Governmental Organization|null      |false   |Wekerom-Riemterdijk|31        |pm25     |reference grade|µg/m³|4.491|\n|{52.11162099964508, 5.708418999868416}|NL     |{2024-07-08T14:00:00+02:00, 2024-07-08T12:00:00+00:00}|Governmental Organization|null      |false   |Wekerom-Riemterdijk|31        |pm25     |reference grade|µg/m³|2.69 |\n|{52.11162099964508, 5.708418999868416}|NL     |{2024-07-08T13:00:00+02:00, 2024-07-08T11:00:00+00:00}|Governmental Organization|null      |false   |Wekerom-Riemterdijk|31        |pm25     |reference grade|µg/m³|4.952|\n|{52.11162099964508, 5.708418999868416}|NL     |{2024-07-08T12:00:00+02:00, 2024-07-08T10:00:00+00:00}|Governmental Organization|null      |false   |Wekerom-Riemterdijk|31        |pm25     |reference grade|µg/m³|7.682|\n+--------------------------------------+-------+------------------------------------------------------+-------------------------+----------+--------+-------------------+----------+---------+---------------+-----+-----+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Transformation: Adding a new column \"quality_label\" based on 'value'\ndef quality_label(value):\n    value = row['value']\n    parameter = row['parameter']\n    \n    if parameter == 'nox':\n        if value < 40:\n            return 'Good'\n        elif value < 80:\n            return 'Moderate'\n        else:\n            return 'Unhealthy'\n    elif parameter == 'pm25':\n        if value < 12:\n            return 'Good'\n        elif value < 35.4:\n            return 'Moderate'\n        else:\n            return 'Unhealthy'\n    elif parameter == 'pm10':\n        if value < 50:\n            return 'Good'\n        elif value < 150:\n            return 'Moderate'\n        else:\n            return 'Unhealthy'\n    elif parameter == 'no2':\n        if value < 40:\n            return 'Good'\n        elif value < 80:\n            return 'Moderate'\n        else:\n            return 'Unhealthy'\n    elif parameter == 'so2':\n        if value < 40:\n            return 'Good'\n        elif value < 80:\n            return 'Moderate'\n        else:\n            return 'Unhealthy'\n    elif parameter == 'o3':\n        if value < 50:\n            return 'Good'\n        elif value < 100:\n            return 'Moderate'\n        else:\n            return 'Unhealthy'\n    elif parameter == 'no':\n        if value < 40:\n            return 'Good'\n        elif value < 100:\n            return 'Moderate'\n        else:\n            return 'Unhealthy'\n    elif parameter == 'co':\n        if value < 5:\n            return 'Good'\n        elif value < 10:\n            return 'Moderate'\n        else:\n            return 'Unhealthy'\n\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\nquality_label_udf = udf(quality_label, StringType())\n\n# More data exploration: Count of data points per country\ncountry_counts = df.groupBy(\"country\").count().orderBy(col(\"count\").desc())\ncountry_counts.show()\n\n# More data exploration: Average value per city\ncity_avg = df.groupBy(\"city\").agg({\"value\": \"avg\"}).orderBy(col(\"avg(value)\").desc())\ncity_avg.show()\n\n# At the end of the exploration and transformation, you can choose to write the data back to S3 if needed.\n# For demonstration purposes, I've commented out the write step below. You can uncomment it to save the data.\noutput_location = \"s3://myawasbuckt-airquality/openaq/processed_data/\"",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------+-----+\n|country|count|\n+-------+-----+\n|     GB| 4781|\n|     NL| 2722|\n|     CL| 1029|\n|     US|  345|\n|     BE|  171|\n+-------+-----+\n\n+--------------------+------------------+\n|                city|        avg(value)|\n+--------------------+------------------+\n|           Talagante| 93.38121546961327|\n|              Curicó| 68.82222222222222|\n|            Punteras|44.899441340782126|\n|    Ealing Horn Lane|31.385792349726778|\n|London Haringey P...|29.820508105849584|\n|   London Hillingdon| 27.08360950276243|\n|           Cauquenes| 19.94743016759777|\n|London Marylebone...| 19.13181081081081|\n|   London Bloomsbury|18.189461848484846|\n|London N. Kensington|17.498828573407202|\n|   Haringey Roadside|17.214494535519123|\n|    Zaanstad-Hemkade| 16.84562500000001|\n|      Coya Población|15.467584269662916|\n|   London Harlington|14.852347256410257|\n|Hoek v. Holland-B...|14.196249999999997|\n|Tower Hamlets Roa...|14.026051912568306|\n|     Camden Kerbside|12.731212049335863|\n|        Coyhaique II| 12.61999999999996|\n|Amsterdam-Van Die...| 11.62091503267974|\n|Wijk aan Zee-Burg...|10.438095238095237|\n+--------------------+------------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		}
	]
}